{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIGI405-19S2 - Web Scraping Lab\n",
    "\n",
    "This lab's activities introduces scraping using:  \n",
    "\n",
    "1. the Google Chrome Web Scraper plugin to automate the browser to crawl a site and extract relevant data;\n",
    "2. Python code to request and parse an HTML page using BeautifulSoup.  \n",
    "\n",
    "One of the three notebooks you will need to complete for Lab exercise 2 is an extension exercise based on the python code we work with today. The notebook will be available on Learn in time for this week's labs.\n",
    "\n",
    "## Web scraping with webscraper.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, install the the [Web Scraper](https://chrome.google.com/webstore/detail/webscraper/jnhgnonknehpejjnehehllkliplmbmhn?hl=en-US) extension for Google Chrome.\n",
    "\n",
    "You should also open the Web Scraper website, which has [tutorials](http://webscraper.io/tutorials), and good explanations and links to further resources.\n",
    "\n",
    "Web Scraper integrates with Chrome's Developer Tools so you can scrape web content while taking advantage of the other information available, all within your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you are interested to research media targeted at youths in NZ. You'd like to study the stories on RNZ's [_The Wireless_](https://www.radionz.co.nz/news/the-wireless) website in detail - perhaps using text or network analysis methods. You want some data structured in CSV format in order to explore this in more detail. \n",
    "\n",
    "- Create a new Site Map, name it 'Wireless', and provide the url ```https://www.radionz.co.nz/news/the-wireless```.  \n",
    "- Next, create a 'story_links' selector to collects the links to each news story on the first page. It should have a type 'Link' and the 'Multiple' box should be ticked. \n",
    "- Use the Inspector / Inspect Element function in Chrome Developer tools to identify a CSS selector to identify news stories. Hint: it will be a 'class' attribute.\n",
    "- If you can't find a selector by looking at the HTML, try using the 'Select' button in Web Scraper to graphically select one of the stories. Test the result by clicking 'Element Preview' and 'Data Preview'.\n",
    "\n",
    "If you get stuck at this point, ask a neighbour, or your tutor can help.\n",
    "\n",
    "Next, in the main browswer window click on one of the story links - any one should do - so that you are now viewing a single news story page. In your Web Scraper add on, click into ```story_links``` (ie click it's name in the ID column). You are now ready to create some selectors for the content of each story.\n",
    "\n",
    "Create the following selectors:\n",
    "\n",
    "* title (Text)\n",
    "* date (Text)\n",
    "* story_text (Text) \n",
    "\n",
    "Note: be careful when selecting the story_text to ensure you are getting the full article body. You can use the 'Element Preview' and 'Data Preview' to check this.\n",
    "\n",
    "After you've created these, your sitemap graph should look something like this:\n",
    "\n",
    "![](selector_graph.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automating the data collection from multiple pages will be important for collecting lots of data.\n",
    "\n",
    "To scrape more than one page of content at a time, we create a `pagination` selector as a child of `_root`, and give it a type of Link. Additionally,\n",
    "\n",
    "* Both the Selector and Click Selector will be the 'Next Page' button.\n",
    "\n",
    "Save your new `pagination` selector and then edit it again. Here you need to Control-Click to add `pagination` as a second parent selector of itself. This will ensure it follows all the pagination links.\n",
    "\n",
    "Then edit the `story_links` selector. Add `pagination` as a second parent selector to `story_links`. This will ensure that every time the 'Next Page' button is 'clicked' by the scraper, it will then add all the stories it subsequently finds.\n",
    "\n",
    "Your selector graph should now look like this:\n",
    "\n",
    "![](revised-selector-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some data\n",
    "\n",
    "Run the scraper by going to Sitemap Wireless > Scrape. Let it run for a couple of minutes. If the scraper is still running after a long time and you want to move on, stop it by closing the popup window where it loads each story page.\n",
    "\n",
    "Once you see clean data in the Browse view, you can export it to a CSV file that can be imported into Excel, or loaded into Python using the code below.\n",
    "\n",
    "You can run this code to load data from a sample file. You should change this to the file created when you ran your scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with open('sample-wireless-news.csv', encoding='utf-8') as f:\n",
    "    df = pd.read_csv(f) # read csv into a pandas dataframe\n",
    "df.head(5) # display the first five rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending your RNZ scraper\n",
    "\n",
    "A key part of scraping is inspecting websites closely, scoping out what information is available and working out how best you can retrieve the information you want. \n",
    "\n",
    "RNZ uses the same styling through their site, so you can repurpose your scraper to retrieve data from other parts of the RNZ website. For example, if you edit the metadata for your scraper you can change the start URL to crawl another major site section (e.g. their 'Comment & Analysis' section https://www.rnz.co.nz/news/on-the-inside).\n",
    "\n",
    "Take a look at how many pages of results there are for the Comment & Analysis section. It is the same as The Wireless. One limitation of crawling main site sections on RNZ's site is that it will only show the most recent results (up to 9 pages). \n",
    "\n",
    "You can make use of other site functionality though to access older content on the RNZ site by changing the start URL to crawl a tag (e.g. https://www.rnz.co.nz/tags/internet) or specific search results (e.g. https://www.rnz.co.nz/search/results?utf8=%E2%9C%93&q=climate+change&commit=Search). Take a look at those pages and then use the 'Element Preview' feature to confirm that the relevant links from your scraper are highlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python for Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth exploring how we can use Python to create scripts for web scraping. Although writing your own web scraper may initially be slower than using a program like webscraper.io, ultimately using Python gives us the most flexibility. Coding a scraper is much more powerful, allowing you to capture the data you want and process it or export it how you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main Python libraries that we'll use to do web scraping are:\n",
    "\n",
    "* [Requests](http://docs.python-requests.org/en/master/) - for requesting web pages\n",
    "* [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) - for parsing (reading) the HTML and selecting the elements we want\n",
    "\n",
    "This code will request [The Wireless page](https://www.rnz.co.nz/news/the-wireless) we have been working with so far in the lab and print it out so you can inspect the HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the requests module\n",
    "import requests\n",
    "\n",
    "# define the url we want as a string\n",
    "url = 'https://www.rnz.co.nz/news/the-wireless'\n",
    "\n",
    "# make the request and assign the result to a variable 'response'\n",
    "response = requests.get(url)\n",
    "\n",
    "# the data will be stored in response.text\n",
    "# if response.text exists, and print it if it does\n",
    "if response.text:\n",
    "    print(response.text)\n",
    "    \n",
    "# Scroll through the resulting HTML, which will be pretty hard to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup to the rescue..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup is a library for parsing HTML (and XML). It is incredibly useful and fairly easy to learn. The documentation pages (linked above) note that it was used to make [this artwork](http://www.nytimes.com/2007/10/25/arts/design/25vide.html), which is an interesting example for its use of both digital and analogue media.\n",
    "\n",
    "Beautiful Soup provides methods for accessing HTML elements, and their useful attributes such as `id` and `class` attributes.\n",
    "\n",
    "Here is an example of retrieving the title element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# create a BeautifulSoup object using the html.parser\n",
    "soup = BeautifulSoup(response.text, \"html.parser\") \n",
    "# find the html title tag\n",
    "title = soup.title\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, you would probably just want the text ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(title.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find by class attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we find links to the list of stories? \n",
    "\n",
    "Using the find_all filter you can return specific element types and target them more specifically using their class attribute. This code will each ```<li>``` with the class attribute containing ```o-digest```. You can check the HTML above to verify that these elements contain a story link. \n",
    "\n",
    "Note: `class_` with an underscore is used because class is a reserved word in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = soup.find_all('li', class_='o-digest')\n",
    "\n",
    "for story in stories:\n",
    "    link = story.a #we want the a element that is a child of the list item\n",
    "    print(link['href']) #we just want to see the URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the Web Scraper for Chrome, we can also use a CSS selector that identifies each story in the HTML. The code below uses `.select()` to target the elements we want using a CSS selector. So, if you can learn about CSS selectors you have a powerful tool to extract the data you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = soup.select('li.o-digest') # this is elegant and flexible!\n",
    "\n",
    "for story in stories:\n",
    "    link = story.a\n",
    "    print(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find by regular expression\n",
    "\n",
    "We can use regular expressions to target specific content in the page. For example, if we just wanted all the \"Wireless Docs\" links we can select only links with that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex_results = soup.find_all(string=re.compile(\"Wireless Docs:\"))\n",
    "for result in regex_results:\n",
    "    link = result.find_parent('a', class_='faux-link') # just want the relevant links\n",
    "    if link is not None:\n",
    "        print(link.get_text(),link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Continue investigating _The Wireless_ and try to copy and modify code from above to:\n",
    "\n",
    "1. Modify the regular expression to list all the 'Someday Stories' articles from https://www.rnz.co.nz/news/the-wireless\n",
    "2. Copy and modify the code to collect all the links in the text of this article: https://www.radionz.co.nz/news/the-wireless/375285/feature-artificial-affection-the-psychology-of-human-robot-interactions \n",
    "3. For the story https://www.radionz.co.nz/news/the-wireless/375285/feature-artificial-affection-the-psychology-of-human-robot-interactions, write code to extract the photo caption text using `.select()` and CSS selectors. \n",
    "4. On the page https://www.rnz.co.nz/news/the-wireless the articles with video are indicated with (VIDEO) after the description. Write some code to find all stories and test whether a story contains video. Your code should output the URL to each story that features video.\n",
    "\n",
    "There is a notebook with solutions for each of the four tasks on Learn. Try your best to write the code using the examples above. You can use the solutions to check your answer or to help you if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finished?\n",
    "\n",
    "This would be a good opportunity to make a start on the week 4 notebook for Lab Exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
