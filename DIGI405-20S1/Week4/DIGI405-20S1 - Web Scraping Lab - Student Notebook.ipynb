{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIGI405 - Web Scraping Lab\n",
    "\n",
    "This lab's activities introduces scraping using:  \n",
    "\n",
    "1. the Google Chrome Web Scraper plugin to automate the browser to crawl a site and extract relevant data;\n",
    "2. Python code to request and parse an HTML page using BeautifulSoup.  \n",
    "\n",
    "\n",
    "## Web scraping with webscraper.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, install the the [Web Scraper](https://chrome.google.com/webstore/detail/webscraper/jnhgnonknehpejjnehehllkliplmbmhn?hl=en-US) extension for Google Chrome.\n",
    "\n",
    "You should also open the Web Scraper website, which has [tutorials](http://webscraper.io/tutorials), and good explanations and links to further resources.\n",
    "\n",
    "Web Scraper integrates with Chrome's Developer Tools so you can scrape web content while taking advantage of the other information available, all within your browser. In Chrome you access Developer Tools with Shift+Ctrl+I, or by finding Developer Tools via the menu. Once Web Scraper is installed it will appear as a tab within the tools:\n",
    "\n",
    "![](web_scraper_tool.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scraper intro video\n",
    "\n",
    "Your tutors should arrange to watch this as a group at the beginning, to avoid chaos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/n7fob_XVsbY\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7f41449e5978>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('n7fob_XVsbY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you are interested to research media targeted at youths in NZ. You'd like to study the stories on RNZ's [_The Wireless_](https://www.radionz.co.nz/news/the-wireless) website in detail - perhaps using text or network analysis methods. You want some data structured in CSV format in order to explore this in more detail. \n",
    "\n",
    "- Create a new Site Map, name it 'wireless', and provide the url ```https://www.radionz.co.nz/news/the-wireless```.  \n",
    "- Next, create a 'story_links' selector to collects the links to each news story on the first page. It should have a type 'Link' and the 'Multiple' box should be ticked. \n",
    "- Use the Element tab in Chrome Developer tools to identify a CSS selector to identify news stories. Hint: it will be a 'class' attribute. The quickest way is to right-click on some part of a page, select 'Inspect', and Chrome will show you the corresponding HTML.\n",
    "- If you can't find a selector by looking at the HTML, try using the 'Select' button in Web Scraper to graphically select one of the stories. Test the result by clicking 'Element Preview' and 'Data Preview':\n",
    "\n",
    "![Element and Data Preview](data_preview.jpg)\n",
    "\n",
    "If you get stuck at this point, ask a neighbour, or your tutor can help. A notebook with solutions will be available on Learn, but try to figure it out in other ways first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect contents of each story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in the main browswer window click on one of the story links - any one should do - so that you are now viewing a single news story page. In your Web Scraper add on, click into ```story_links``` (ie click its name in the ID column). You are now ready to create some selectors for the content of each story.\n",
    "\n",
    "Create the following selectors:\n",
    "\n",
    "* title (Text)\n",
    "* date (Text)\n",
    "* story_text (Text) \n",
    "\n",
    "Note: be careful when selecting the story text to ensure you are getting the full article body. You can use the 'Element Preview' and 'Data Preview' to check this.\n",
    "\n",
    "After you've created these, your sitemap graph should look something like this:\n",
    "\n",
    "![](selector_graph.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automating the data collection from multiple pages will be important for collecting lots of data.\n",
    "\n",
    "To scrape more than one page of content at a time, we create a `pagination` selector as a child of `_root`, and give it a type of Link. You also need to Control-Click to add `pagination` as a second parent selector of itself. Finally, edit the `story_links` selector and make `pagination` a second parent selector to `story_links`. \n",
    "\n",
    "This will make our scraper recursively check for more story links and pagination links, and continue following them. Every time 'Next Page' button is 'clicked' by the scraper, it will then find all the story links and pagination links it can, and it will continue this search until it there are no more pages to scrape.\n",
    "\n",
    "Your selector graph will look more complex now, but don't worry. It's just the recursive (repeated looping) structure that makes it so. The graph should now look like this:\n",
    "\n",
    "![](revised-selector-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some data\n",
    "\n",
    "Run the scraper by going to Sitemap Wireless > Scrape. Let it collect several pages, then stop it by closing the popup window where it loads each story page. Press the 'refresh' button to see your data:\n",
    "\n",
    "![refresh button](refresh.jpg)\n",
    "\n",
    "Once you see clean data in the Browse view, you can export it to a CSV file that can be imported into Excel, or loaded into Python using the code below.\n",
    "\n",
    "You can run this code to load data from a sample file. You should change this to the file created when you ran your scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with open('sample-wireless-news.csv', encoding='utf-8') as f:  # change the file name for your file\n",
    "    df = pd.read_csv(f) # read csv into a pandas dataframe\n",
    "df.head(5) # display the first five rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export script 1 - Text type selector\n",
    "\n",
    "Use the script below to export your scraped content to a directory of text files *if your text column contains plain text*. \n",
    "\n",
    "Please note the following:\n",
    "\n",
    "- this will only work with data in the CSV format exported from webscraper.io\n",
    "- you should inspect the webscraper output in CSV format first, to save repeating this process if changes are needed\n",
    "- you must load the CSV into this notebook as a pandas dataframe using the cell above FIRST\n",
    "- you must create a directory called 'textfiles' in the same directory as this notebook (or wherever you run the code from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once your data is loaded in the cell above and you've created a 'textfiles' directory, run this cell    \n",
    "\n",
    "text_column_name = 'story_text' #modify this if your column is named something else\n",
    "\n",
    "for idx, col in df.iterrows():   \n",
    "    \n",
    "    with open('textfiles/{}.txt'.format(str(idx)), 'w', encoding='utf-8') as f:\n",
    "            try:\n",
    "                f.write(col[text_column_name])\n",
    "                print('Writing file ' + str(idx))\n",
    "            except:\n",
    "                pass # this is a quick workaround for any empty cells - they'll be ignored.\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export script 2 - Grouped type selector\n",
    "\n",
    "Use the script below to export your scraped content to a directory of text files *if your text column contains JSON*. To capture mutliple paragraphs, you may need to use the 'Grouped' selector in the Webscraper, which outputs JSON data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once your data is loaded in the cell above and you've created a 'textfiles' directory, run this cell    \n",
    "import json\n",
    "\n",
    "text_column_name = 'story_text' #modify this if your column is named something else\n",
    "\n",
    "for idx, col in df.iterrows():   \n",
    "    \n",
    "    with open('textfiles/{}.txt'.format(str(idx)), 'w', encoding='utf-8') as f:\n",
    "        try:\n",
    "            chunks = json.loads(col[text_column_name]) # this parses the json structure\n",
    "            for chunk in chunks:\n",
    "                f.write(chunk[text_column_name])\n",
    "            print('Writing file ' + str(idx))\n",
    "        except:\n",
    "            pass # this is a quick workaround for any empty cells - they'll be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending your RNZ scraper\n",
    "\n",
    "A key part of web scraping is inspecting websites closely, scoping out what information is available and working out how best you can retrieve the information you want. \n",
    "\n",
    "RNZ uses the same styling through their site, so you can repurpose your scraper to retrieve data from other parts of the RNZ website. For example, if you edit the metadata for your scraper you can change the start URL to crawl another major site section (e.g. their 'Comment & Analysis' section https://www.rnz.co.nz/news/on-the-inside).\n",
    "\n",
    "Take a look at how many pages of results there are for the Comment & Analysis section. It is the same as The Wireless. One limitation of crawling main site sections on RNZ's site is that it will only show the most recent results (up to 9 pages). \n",
    "\n",
    "You can make use of other site functionality though to access older content on the RNZ site by changing the start URL to crawl a tag (e.g. https://www.rnz.co.nz/tags/internet) or specific search results (e.g. https://www.rnz.co.nz/search/results?utf8=%E2%9C%93&q=climate+change&commit=Search). Take a look at those pages and then use the 'Element Preview' feature to confirm that the relevant links from your scraper are highlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python for Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth exploring how we can use Python to create scripts for web scraping. Although writing your own web scraper may initially be slower than using a program like webscraper.io, ultimately using Python gives us the most flexibility. Coding a scraper is much more powerful, allowing you to capture the data you want and process it or export it how you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main Python libraries that we'll use to do web scraping are:\n",
    "\n",
    "* [Requests](http://docs.python-requests.org/en/master/) - for requesting web pages\n",
    "* [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) - for parsing (reading) the HTML and selecting the elements we want\n",
    "\n",
    "This code will request [The Wireless page](https://www.rnz.co.nz/news/the-wireless) we have been working with so far in the lab and print it out so you can inspect the HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the requests module\n",
    "import requests\n",
    "\n",
    "# define the url we want as a string\n",
    "url = 'https://www.rnz.co.nz/news/the-wireless'\n",
    "\n",
    "# make the request and assign the result to a variable 'response'\n",
    "response = requests.get(url)\n",
    "\n",
    "# the data will be stored in response.text\n",
    "# if response.text exists, and print it if it does\n",
    "if response.text:\n",
    "    print(response.text)\n",
    "    \n",
    "# Scroll through the resulting HTML, which will be pretty hard to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup to the rescue..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup is a library for parsing HTML (and XML). It is incredibly useful and fairly easy to learn. The documentation pages (linked above) note that it was used to make [this artwork](http://www.nytimes.com/2007/10/25/arts/design/25vide.html), which is an interesting example for its use of both digital and analogue media.\n",
    "\n",
    "Beautiful Soup provides methods for accessing HTML elements, and their useful attributes such as `id` and `class` attributes.\n",
    "\n",
    "Here is an example of retrieving the title element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# create a BeautifulSoup object using the html.parser\n",
    "soup = BeautifulSoup(response.text, \"html.parser\") \n",
    "# find the html title tag\n",
    "title = soup.title\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, you would probably just want the text ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(title.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find by class attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we find links to the list of stories? \n",
    "\n",
    "Using the find_all filter you can return specific element types and target them more specifically using their class attribute. This code will each ```<li>``` with the class attribute containing ```o-digest```. You can check the HTML above to verify that these elements contain a story link. \n",
    "\n",
    "Note: `class_` with an underscore is used because class is a reserved word in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = soup.find_all('li', class_='o-digest')\n",
    "\n",
    "for story in stories:\n",
    "    link = story.a #we want the a element that is a child of the list item\n",
    "    print(link['href']) #we just want to see the URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.find_all()` method uses CSS selectors just like those used in the webscraper.io browser plugin above. However, BeautifulSoup also has a `.select()` method, which can be more flexible (and looks nicer!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # this class selector captures the lead story also (which is in a div element, not a li element)\n",
    "stories = soup.select('.o-digest')\n",
    "\n",
    "for story in stories:\n",
    "    link = story.a\n",
    "    print(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find by regular expression\n",
    "\n",
    "We can use regular expressions to target specific content in the page.\n",
    "\n",
    "For example, if we just wanted all the \"Wireless Docs\" links we can select only links with that text. In the example in the next cell, `re.compile(\"Wireless Docs:\")` defines the regular expression. There is only one string we want to match here, but if we wanted to match \"Wireless Docs:\" and \"Wireless Audio:\" we could write an expression like: `re.compile(\"Wireless\\s[A-Z][a-z]+\\:\")` to match\n",
    "\n",
    "- \"Wireless\"\n",
    "- \\s = a space\n",
    "- [A-Z] = a capital letter\n",
    "- [a-z] = a lowercase letter\n",
    "- ```+``` = 1 or more of the preceding character (in this case, lowercase letters)\n",
    "- a colon (':') character\n",
    "\n",
    "If we check this using [Regex101](https://regex101.com/) (selecting Python flavour regex) we should see this in action:\n",
    "![Regex 101 example](regex_wireless.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex_results = soup.find_all(string=re.compile(\"Wireless Docs:\")) # just Wireless Docs to match here\n",
    "\n",
    "for result in regex_results:\n",
    "    link = result.find_parent('a', class_='faux-link') # just want the relevant links\n",
    "    if link is not None:\n",
    "        print(link.get_text(),link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Continue investigating _The Wireless_ and try to copy and modify code from above to:\n",
    "\n",
    "1. Modify the regular expression to list all the 'Someday Stories' articles from https://www.rnz.co.nz/news/the-wireless\n",
    "2. Copy and modify the code to collect all the links in the text of this article: https://www.radionz.co.nz/news/the-wireless/375285/feature-artificial-affection-the-psychology-of-human-robot-interactions. Hint: You'll want to use `soup.find()` rather than `soup.find_all()` to get the body of the article, since you're looking for one thing rather than a list of things.\n",
    "3. For the story https://www.radionz.co.nz/news/the-wireless/375285/feature-artificial-affection-the-psychology-of-human-robot-interactions, write code to extract the photo caption text using `.select()` and CSS selectors. \n",
    "4. On the page https://www.rnz.co.nz/news/the-wireless the articles with video are indicated with (VIDEO) after the description. Write some code to find all stories and test whether a story contains video. Your code should output the URL to each story that features video.\n",
    "\n",
    "There is a notebook with solutions for each of the four tasks on Learn. Try your best to write the code using the examples above. You can use the solutions to check your answer or to help you if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finished?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another web scraping exercise\n",
    "\n",
    "Try scraping blog entries from the [Food & Grocery Council blog page](https://www.fgc.org.nz/category/news/fgc-blogs/). News and media releases from organisations that advocate or lobby for certain interests are an interesting source of texts. A contrasting view with the Food & Grocery Council, who work on behalf of food distributors and retailers in NZ might be a group advocating reducing plastic waste, or reducing food miles. This might be the start of a comparison where you could use keyness analysis. \n",
    "\n",
    "Hint: Watch the [Webscraper.io pagination tutorial](https://www.youtube.com/watch?v=x8bZmUrJBl0) first. For this site you can use the first method (figure out how many pages and then specify [1-x] in the starting URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Or ...\n",
    "\n",
    "See if you can use your new web scraping skills on your Corpus Analysis assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
