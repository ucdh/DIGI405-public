{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIGI405-20S1 - Topic Modeling TED.com transcripts\n",
    "\n",
    "This notebook introduces Gensim for topic modeling. The 2018 TED.com transcripts are also available for download on the datasets page on Learn if you wish to train models using TMT.\n",
    "\n",
    "Work through the notebook. The key things to do are:\n",
    "1. to try training some different size models (e.g. 10 topics, 30 topics, 50 topics);  \n",
    "2. to explore the topic assignments for documents and assess the quality of topics returned; \n",
    "3. to measure 'c_v' topic coherence for a number of models;\n",
    "3. to make notes on your observations of different models and the kinds of similarities between documents they produce.\n",
    "\n",
    "Since we need to evaluate topic models against a use case - think about the idea of a recommendation engine: what model performs best for finding similiar TED talks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim import similarities\n",
    "\n",
    "import os.path\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "mallet_path = '/opt/mallet-2.0.8/bin/mallet' # this should be the correct path for the DIGI405 lab workrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to install nltk run this in Anaconda prompt: pip install nltk \n",
    "# note if you get an error with stopwords below then uncomment the following lines and rerun this cell \n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells contain functions to load a corpus from a directory of text files, preprocess the corpus and create the bag of words document-term matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_dir(path):\n",
    "    file_list = glob.glob(path + '/*.txt')\n",
    "\n",
    "    # create document list:\n",
    "    documents_list = []\n",
    "    for filename in file_list:\n",
    "        with open(filename, 'r', encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "            f.close()\n",
    "            documents_list.append(text)\n",
    "    print(\"Total Number of Documents:\",len(documents_list))\n",
    "    return documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set,extra_stopwords = {}):\n",
    "    # adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
    "    # replace all newlines or multiple sequences of spaces with a standard space\n",
    "    doc_set = [re.sub('\\s+', ' ', doc) for doc in doc_set]\n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # add any extra stopwords\n",
    "    if (len(extra_stopwords) > 0):\n",
    "        en_stop = en_stop.union(extra_stopwords)\n",
    "    \n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # add tokens to list\n",
    "        texts.append(stopped_tokens)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean):\n",
    "    # adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    \n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "    return dictionary,doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and pre-process the corpus\n",
    "Load the corpus, preprocess with additional stop words and output dictionary and document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the path below to wherever you have the transcripts2018 folder\n",
    "document_list = load_data_from_dir(\"data/transcripts2018/\")\n",
    "\n",
    "# I've added extra stopwords here in addition to NLTK's stopword list - you could look at adding others.\n",
    "doc_clean = preprocess_data(document_list,{'laughter','applause'})\n",
    "\n",
    "dictionary, doc_term_matrix = prepare_corpus(doc_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA model with 20 topics\n",
    "The following cell sets the number of topics we are training the model for. The one after trains the model and outputs the topics. Note: this can take a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics=20 # adjust this to alter the number of topics\n",
    "words=20 #adjust this to alter the number of words output for the topic below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs LDA using Mallet from gensim using the number_of_topics specified above - this might take a couple of minutes\n",
    "# you can create additional variables eg ldamallet20 to store models with different numbers of topics\n",
    "ldamallet20 = LdaMallet(mallet_path, corpus=doc_term_matrix, num_topics=number_of_topics, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the topics\n",
    "ldamallet20.show_topics(num_topics=number_of_topics,num_words=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Gensim model format\n",
    "Convert the Mallet model to gensim format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensimmodel20 = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherencemodel = CoherenceModel(model=gensimmodel20, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "print (coherencemodel.get_coherence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate coherence scores for models with different numbers of topics\n",
    "\n",
    "You should create some further models below to test different numbers of topics.\n",
    "\n",
    "For example, the code block below will train a model with 30 topics and return the coherence score. You can duplicate this and change the number of topics and the variable names to keep track of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet30 = LdaMallet(mallet_path, corpus=doc_term_matrix, num_topics=30, id2word=dictionary)\n",
    "gensimmodel30 = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet30)\n",
    "coherencemodel = CoherenceModel(model=gensimmodel30, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "print (coherencemodel.get_coherence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a range of topic sizes and plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: this process will take a while to run! Make sure you have tried a number topic sizes to get a sense of what models you need to test. I suggest you test no more than 8-10 models using the code below, so as not to be waiting too long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supply values for k and the interval, eg 20, 60, 10 will train models for 20, 30, 40, 50, and 60 topics\n",
    "min_k = \n",
    "max_k = \n",
    "intervals = \n",
    "\n",
    "coherences = {}\n",
    "\n",
    "for i in range(min_k, max_k, intervals):\n",
    "    ldamalletmodel = LdaMallet(mallet_path, corpus=doc_term_matrix, num_topics=i, id2word=dictionary)\n",
    "    gensimmodel = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamalletmodel)\n",
    "    coherences[i] = CoherenceModel(model=gensimmodel, texts=doc_clean, dictionary=dictionary, coherence='c_v').get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the coherence scores to a pandas dataframe\n",
    "df = pd.DataFrame.from_dict(coherences, orient='index', columns=['Coherence'])\n",
    "df['Topics'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result\n",
    "df.plot(kind='line', x='Topics', y='Coherence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview a document\n",
    "\n",
    "Preview a document - you can change the doc_id to view another document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 10 # index of document to explore\n",
    "print(re.sub('\\s+', ' ', document_list[doc_id])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the distribution of topics for the document\n",
    "\n",
    "The next cell outputs the distribution of topics on the document specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topics = gensimmodel20.get_document_topics(doc_term_matrix[doc_id])\n",
    "document_topics = sorted(document_topics, key=lambda x: x[1], reverse=True) # sorts document topics\n",
    "\n",
    "for topic, prop in document_topics:\n",
    "    topic_words = [word[0] for word in gensimmodel20.show_topic(topic, 10)]\n",
    "    print (\"%.2f\" % prop, topic, topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find similar documents\n",
    "This will find the 5 most similar documents to the document specified above based on their topic distribution. The MatrixSimilarity() method uses cosine similarity to measure how similar the document specified by `docid` is to all other documents for that model. There are better measures, but this one is quick and simple to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_index = similarities.MatrixSimilarity(doc_term_matrix)\n",
    " \n",
    "# query for our doc_id from above\n",
    "similarity_index = lda_index[doc_term_matrix[doc_id]]\n",
    "# Sort the similarity index\n",
    "similarity_index = sorted(enumerate(similarity_index), key=lambda item: -item[1])\n",
    "\n",
    "for i in range(1,6): \n",
    "    document_id, similarity_score = similarity_index[i]\n",
    "    print('Document Index: ',document_id)\n",
    "    print('Similarity Score',similarity_score)\n",
    "    print(re.sub('\\s+', ' ', document_list[document_id][:500]), '...') # preview first 500 characters\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
